{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BiasBERT_Kurita.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMXNuKa84pOPaQuzK6s26hy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eunyhwang/Debiasing_BERT/blob/main/BiasBERT_edited.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uM5BqfCd5L9V"
      },
      "source": [
        "1. Load a word embedding model using the genism API.\n",
        "Gensim is a Python library for topic modelling, document indexing and similarity retrieval with large corpora. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbgpBziV5D0A",
        "outputId": "285804d3-80dd-4b38-b7d2-94ba900f8330"
      },
      "source": [
        "#!pip install --upgrade gensim\n",
        "import gensim.downloader as api"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
            "  warnings.warn(msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6uL-tCF6UIL"
      },
      "source": [
        "2. Create the Query object using the target words (male names and female names) and two attribute words sets (Career and Family terms) # modify later"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwQiaT9O5kwq"
      },
      "source": [
        "#target sets (Male/Female names)\n",
        "male_names = ['John', 'Paul', 'Mike', 'Kevin', 'Steve', 'Greg', 'Jeff', 'Bill']\n",
        "female_names = ['Amy', 'Joan', 'Lisa', 'Sarah', 'Diana', 'Kate', 'Ann', 'Donna']\n",
        "\n",
        "#attribute sets (Career/Family titles)\n",
        "career = ['executive', 'management', 'professional', 'corporation',\n",
        "         'salary', 'office', 'business', 'career']\n",
        "family = ['home', 'parents', 'children', 'family', 'cousins', 'marriage',\n",
        "         'wedding', 'relatives']\n",
        "\n",
        "#query (T1, T2, A1, A2) order is not important\n",
        "gender_occupation_query = Query([male_names, female_names],\n",
        "                                [career, family],\n",
        "                                ['Male names', 'Female names'],\n",
        "                                ['Career', 'Family'])\n",
        "                      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1YHqqZXDhrm"
      },
      "source": [
        "import os\n",
        "import json\n",
        "import argparse # parser for command-line options, arguments and sub-commands\n",
        "\n",
        "\n",
        "def main(args):\n",
        "    weat_dict = dict()\n",
        "\n",
        "    # build weat_dict\n",
        "    for data_name in os.listdir(args.weat_dir):\n",
        "        path = os.path.join(args.weat_dir, data_name)\n",
        "\n",
        "        if os.path.abspath(path) == os.path.abspath(args.output):\n",
        "            continue\n",
        "\n",
        "        data_dict = dict()\n",
        "        weat_dict[data_name] = data_dict\n",
        "        keys = []\n",
        "\n",
        "        with open(path) as f:\n",
        "            for line in f.readlines():\n",
        "                if not line.strip():\n",
        "                    continue\n",
        "\n",
        "                key, values = line.split(':')\n",
        "                key = key.strip()\n",
        "                values = [w.strip().lower() for w in values.split(',')]\n",
        "\n",
        "                data_dict[key] = values\n",
        "                keys.append(key)\n",
        "\n",
        "        if len(keys) == 3:\n",
        "            data_dict['method'] = 'wefat'\n",
        "\n",
        "            data_dict['W_key'] = keys[0]\n",
        "            data_dict['A_key'] = keys[1]\n",
        "            data_dict['B_key'] = keys[2]\n",
        "\n",
        "            data_dict['targets'] = '{}'.format(keys[0])\n",
        "            data_dict['attributes'] = '{} vs {}'.format(keys[1], keys[2])\n",
        "\n",
        "        elif len(keys) == 4:\n",
        "            data_dict['method'] = 'weat'\n",
        "\n",
        "            data_dict['X_key'] = keys[0]\n",
        "            data_dict['Y_key'] = keys[1]\n",
        "            data_dict['A_key'] = keys[2]\n",
        "            data_dict['B_key'] = keys[3]\n",
        "\n",
        "            data_dict['targets'] = '{} vs {}'.format(keys[0], keys[1])\n",
        "            data_dict['attributes'] = '{} vs {}'.format(keys[2], keys[3])\n",
        "\n",
        "    with open(args.output, 'w') as f:\n",
        "        json.dump(weat_dict, f, sort_keys=True, indent=4)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--weat_dir', type=str, default='weat/', required=True,\n",
        "                        help='WEAT data directory')\n",
        "    parser.add_argument('--output', type=str, default='weat.json', required=True,\n",
        "                        help='Output JSON file path')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    main(args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLpYJbErkWf9"
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "\n",
        "def square_rooted(x):\n",
        "    return math.sqrt(sum([a*a for a in x]))\n",
        "\n",
        "\n",
        "def cosine_similarity(x,y):\n",
        "    numerator = sum(a*b for a,b in zip(x,y))\n",
        "    denominator = square_rooted(x)*square_rooted(y)\n",
        "    return numerator/float(denominator)\n",
        "\n",
        "def weat_association(W, A1, A2):\n",
        "    \"\"\"\n",
        "    Returns association of the word w in W with the attribute for WEAT score. \n",
        "    param W: target words' vecotr representations\n",
        "    param A1, A2: attribute words' vector representations\n",
        "    return: (len(W), ) shaped numpy ndarray. each rows represet association of the word w in W\n",
        "    \"\"\"\n",
        "    return np.mean(cosine_similarity(W, A), axis=-1) - np.mean(cosine_similarity(W, A2), axis=-1)\n",
        "\n",
        "def weat_score(X, Y, A, B):\n",
        "    \"\"\"\n",
        "    Returns WEAT score\n",
        "    X, Y, A, B must be (len(words), dim) shaped numpy ndarray\n",
        "    CAUTION: this function assumes that there's no intersection word between X and Y\n",
        "    :param X: target words' vector representations\n",
        "    :param Y: target words' vector representations\n",
        "    :param A: attribute words' vector representations\n",
        "    :param B: attribute words' vector representations\n",
        "    :return: WEAT score\n",
        "    \"\"\"\n",
        "    x_association = weat_association(X, A, B)\n",
        "    y_association = weat_association(Y, A, B)\n",
        "\n",
        "    tmp1 = np.mean(x_association, axis=-1) - np.mean(y_association, axis=-1)\n",
        "    tmp2 = np.std(np.concatenate((x_association, y_association), axis=0))\n",
        "\n",
        "    return tmp1 / tmp2\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ennzdNOP0PbP"
      },
      "source": [
        "Bias Calculator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dq04LhMXqa7",
        "outputId": "fa051223-76df-4be6-94d2-68e73c908faa"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.10.0-py3-none-any.whl (2.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 6.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 48.2 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub>=0.0.12\n",
            "  Downloading huggingface_hub-0.0.16-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 5.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 52.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 91.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.16 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.10.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hwkf6xSQ0S-Z"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def softmax(arr, axis=1): #calculating the scores which allows the higher correlation of the hidden state to have a larger fractional value. \n",
        "  e = np.exp(arr)\n",
        "  return e / e.sum(axis=axis, keepdims=True)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-BoxRttrgOH"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwfTdeAIrQ0R"
      },
      "source": [
        "# **0. Preprocessing** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlRREOsir0YP"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSphRTWArYls"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelWithLMHead, pipeline #what is the difference between this model with the others?\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\") #bert-base-uncased: This model is not case-sensitive: it doesn't make a difference between english and English.\n",
        "model = AutoModelWithLMHead.from_pretrained(\"bert-base-uncased\") \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVYh7sPRtN7Q"
      },
      "source": [
        "# **1. Bias Score**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2U8t5cIZSfEj",
        "outputId": "ee491c8b-f486-42d2-ce6b-ae8dfc7e604e"
      },
      "source": [
        "# Prepare a template sentence\n",
        "\"He is a programmer.\"\n",
        "\"[Target] is a [Attribute].\"\n",
        "\n",
        "# Replace Target word with [MASK] and compute the probability that BERT assigns \"he\" for the target word.\n",
        "sentence = f\"{tokenizer.mask_token} is a programmer.\"\n",
        "\n",
        "input_ids = tokenizer.encode(sentence, return_tensors='pt')\n",
        "mask_token_index = torch.where(input_ids == tokenizer.mask_token_id)[1]\n",
        "\n",
        "token_logits = model(input_ids)[0]\n",
        "mask_token_logits = token_logits[0, mask_token_index, :]\n",
        "mask_token_logits = torch.softmax(mask_token_logits, dim=1)\n",
        "\n",
        "'''\n",
        "top_5 = torch.topk(mask_token_logits, 5, dim=1)\n",
        "top_5_tokens = zip(top_5.indices[0].tolist(), top_5.values[0].tolist())\n",
        "\n",
        "for token, score in top_5_tokens:\n",
        "  print(sentence.replace(tokenizer.mask_token, tokenizer.decode([token])), f\"(score: {score})\")\n",
        "'''\n",
        "\n",
        "# Get the probability of token_id\n",
        "target_word = 'he'\n",
        "target_word_id = tokenizer.encode(target_word, add_special_tokens=False)[0] \n",
        "\n",
        "\n",
        "token_prob = mask_token_logits[:, target_word_id].detach().numpy()[0] # extract the prob from tensor (convert into numpy)\n",
        "print(token_prob)\n",
        "\n",
        "#print(f\"Probability of {target_word}: {mask_token_logits[:, target_word_id]}\")\n",
        "\n",
        "# Replace both Target word and Attribute word with [MASK] and compute the probability. \n",
        "\n",
        "sentence_masked = f\"{tokenizer.mask_token} is a {tokenizer.mask_token}.\"\n"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.68536234\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPYEhnsrwlWM"
      },
      "source": [
        "def get_prob(sentence, target_word):\n",
        "  input_ids = tokenizer.encode(sentence, return_tensors='pt')\n",
        "  mask_token_index = torch.where(input_ids == tokenizer.mask_token_id)[1]\n",
        "\n",
        "  token_logits = model(input_ids)[0]\n",
        "  mask_token_logits = token_logits[0, mask_token_index, :] #get the logits\n",
        "  mask_token_logits = torch.softmax(mask_token_logits, dim=1) #to get probability, apply softmax on the logits\n",
        "\n",
        "  target_word_id = tokenizer.encode(target_word, add_special_tokens=False)[0]\n",
        "  token_prob = mask_token_logits[:, target_word_id].detach().numpy()[0]\n",
        "  \n",
        "  return token_prob"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcuMNqXByCoy",
        "outputId": "b335a0e5-50e8-4e02-c168-69cdbb0da892"
      },
      "source": [
        "sample_sentence = f\"{tokenizer.mask_token} is a programmer.\"\n",
        "sample_sentence_masked = f\"{tokenizer.mask_token} is a {tokenizer.mask_token}.\"\n",
        "sample_mw = \"he\"\n",
        "sample_fw = 'she'\n",
        "prob(sample_sentence, sample_mw)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.68536234"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "effTFiZ4yS-l",
        "outputId": "e9bc52c3-681d-4c24-b50a-109ced06eac4"
      },
      "source": [
        "prob(sample_sentence, sample_mw)\n",
        "prob(sample_sentence_masked, sample_mw)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5546252"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZkce8rJywe_"
      },
      "source": [
        "def score(sentence, sentence_masked, target_word):\n",
        "  prob = get_prob(sentence, target_word)\n",
        "  prior_prob = get_prob(sentence_masked, target_word)\n",
        "  association = np.log(prob/prior_prob)\n",
        "\n",
        "  return association"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vfmt8B5F8hZA",
        "outputId": "0bb13f8d-f805-4c93-8c4f-f0d056ebdc2c"
      },
      "source": [
        "score(sample_sentence, sample_sentence_masked, sample_mw)\n",
        "score(sample_sentence, sample_sentence_masked, sample_fw)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.18356283"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5dD8W2085VE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztjpKYp2fYYc"
      },
      "source": [
        "from transformers import pipeline\n",
        "unmasker = pipline(\"fill-mask\", model = \"bert-base-uncased\"\n",
        "unmasker"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}